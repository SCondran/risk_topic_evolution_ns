##Classical Imports
# from time import pthread_getcpuclockid
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import numpy as np
#import sompy # modified as below 11/05/2021
# import src.sompy 
import src.sompy as sompy
import os

from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import NMF, LatentDirichletAllocation
from  sklearn.metrics.pairwise import cosine_similarity
import copy
import operator
import pickle

###Check the following if you need it.
import re
from nltk.corpus import stopwords
import string
#text analytics imports
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer

# sc added
from nltk.sentiment import SentimentAnalyzer
nltk.download('wordnet')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

import time


##Local Imports
import src.features.build_globalVariables as  gv## For global variables
import src.features.build_class as bc ## for defined classes

def dailyCalc(indo_EngTweetsPd_sent, dateList, headerMap,  myNtopics=5, myNfeatures=100, 
              dicObj=None, bfgvFolderLoc = None):
    """
    This function creates an SOM first based on the features extracted from the corpus and then 
    applies the topic modelling on top of the cells in the SOM. The final results returned are 
    a set of topic represented by the keywords. This function is often called in the mainFunction
    in trendy.
    We use the SOMPY package here to transform the tweet matrix into an SOM. However, we wrote a 
    special function in SOMPY to update the SOM without creating a new SOM on each iteration.
    See SOMPY package for details.
    -------------------------------------------------------------------------------------------
    param indo_EngTweetsPd_sent: (required) This are the tweets in the specified period of the
    dateList (startDate - endDate).
    
    param dateList:(required) This is the list of all the dates under consideration between the 
    startDate and endDate. 
    
    param headerMap: This is a dictionary of the format {col:idx} that maps the column names in the 
    tweet dataframe to index numbers so that we can get the right columns of interest even when the 
    column positions are changed.Col is the column name eg 'text', 'sentiment' and idx is a number.
    headermap is generated by the calling function - in this case mainFunction.
    
    Param myNTopics*** (default - 5): This is the number (integer) of topics that we expect to find in 
    each day.
    
    Param myNfeatures*** (default - 100): This is the number of features to extract from the dataset 
    which we will use in the SOM and the topic modelling.
    
    param dicObj : This is the dictionary of the format {date:{T_x:evidenceObject}}. This is read in 
    from mtopicsLoc in teh calling function. When passed in, it means the analysis will update this
    dictionary, else a new one is created and returned back to the calling function. It is then stored in 
    the respective folder (specifed in the calling function)
    
    param bfgvFolderLoc : This is the actual path to the folder containing the global variables
    
    *** - These parameters can affect the result of the analysis - Formal definitions required
    *** - *Mapsize* for som affects the results - formal definition required
    KNOWN ISSUES: When the number of tweets per day is very small typically less than 20 tweets, the som
    takes a really long time for the training and this is because the train len is inversely related to the
    number of samples. However when the number of samples is less than 20, the denominator goes close to 0 and
    the training length becomes close to infinity. We solve this be putting a hard constraint that the number
    of tweets per day should be at least 100 else if it is less then the training length is fixed to 20 
    iterations
    
                : We assume that negative sentiments are <0 (negative) and positive sentiments are >0 
                (positive). This is as per how our senti analyser calculates this. In this function, we 
                invert this value to positive and set the positive sentiments to 0 since we are not 
                interested in the positives. This will not be necessary if the 'negative/bad' sentiment
                are already positive. Or we might need to change the code slightly if the user is interested
                in the positives as well.
    """
    indo_Engdaily_sentiment_Agg = dict()
    FirstSOM = True 
    som = None
    prevFeatureMap = dict()
    t = 0
    
    if dicObj == None:
        dicObj = dict()
    else:
        som = getData_2(os.path.join(bfgvFolderLoc, "som.pkl")) 
        FirstSOM = False
    #DC.1. Above we assign the  SOM to the som variable. If dicObj is specified, it means we are updating 
    print('  running code for date:')
    start_time = time.time()
    for date_ in dateList:
        print('  ' + str(date_) + ' - ' + str(round((time.time() - start_time),2)) + ' seconds')
        indo_EngTweetsPdCur= indo_EngTweetsPd_sent[(indo_EngTweetsPd_sent.publicationDate==date_)]


        if indo_EngTweetsPdCur.empty: 
            print('!!!!!!!!!!!!!! Date skipped: ' + str(date_))    # If no data for a date skip to next date
            continue

    
        
        MyGSR_text = [ twt[headerMap['text']] for twt in indo_EngTweetsPdCur.itertuples()]
        tf,featureNames = TfidfVectorizerMike1(MyGSR_text, n_features=myNfeatures)
        gottenFeatureMapping ={k:featureNames[k]  for k in range(len(featureNames))}
        featureMapping = dict()
        
        tfDense = tf.todense()
        #DC.2. Above converts the sparse matrix to a dense matrix. Since SOMPY requires dense matrix. 
        
        mapsize = [50,50]
        #DC.3. Above we use a 50 x 50 map size. This is a best guess map size. We need to formally define this 

        #***********************Train som for the first time****************************************
        if FirstSOM:
            featureMapping=gottenFeatureMapping
            som = sompy.SOMFactory.build(tfDense, mapsize, mask=None, mapshape='planar', lattice='rect', normalization='var', 
                                         initialization='pca', neighborhood='gaussian', training='batch', name='sompy')  
            
            #DC.4. Above sompy function defined in SOMPY. We use default parameters. See sompy for details.
        #print("beforeTrain:", np.matrix(som.codebook.matrix))
        #print(som)
            if indo_EngTweetsPdCur.count()[0] < 100:
                som.train(n_job=1, verbose=None, train_rough_len=20, train_finetune_len=20)  
                #NOTE: verbose='debug' will print more, 'Info' will print some text and verbose=None wont
                #print anything. We use defaults for now.
                
                #DC.5. Constraint above ensures that days that have less than 100 tweets we fix the 
                #training length otherwise it will take forever due to division by a value close to 0
                sMatrix = np.matrix(som.codebook.matrix)
                sMatrix[np.isnan(sMatrix)] = 0 
            else:
                som.train(n_job=1, verbose=None, train_len_factor=2)                
                sMatrix = np.matrix(som.codebook.matrix)
                sMatrix[np.isnan(sMatrix)] =0
            # print("theFeatures1",featureMapping)
            # print("dataUsed",som._dlen,som._dim)
            # print([method_name for method_name in dir(som)  if callable(getattr(som, method_name))])
                # print("1f")


        #************************************************************************************
        #***********************Update existing som ****************************************
        else:
            UnassignedKeys = list(range(myNfeatures))
            #DC.6. Above stores all the index values that we have used.
            remainderFeatures= list()
            for eachkey in prevFeatureMap.keys():
                if prevFeatureMap[eachkey] in gottenFeatureMapping.values():
                    featureMapping[eachkey] =  prevFeatureMap[eachkey]
                    UnassignedKeys.remove(eachkey)
                    gottenFeatureMapping = { k:v for k, v in gottenFeatureMapping.items() 
                                            if v!= prevFeatureMap[eachkey]}

            for remainterm in gottenFeatureMapping.values():
                temp = UnassignedKeys[0]
                featureMapping[temp] = remainterm
                UnassignedKeys.remove(temp)
            #DC.7. Above removes the expired terms in the feature map and replaces it with new ones. 
            #Remember that the feature map is defined as {word:index} and there is no risk that some index
            #values will never be reassigned since the same number of new features must replace the same
            #number of expired terms.
            som.updateMike(n_job=1, verbose=None, train_len_factor=2, theNewData=tfDense)

            if indo_EngTweetsPdCur.count()[0] < 100:
                som.train(n_job=1, verbose=None, train_rough_len=20,train_finetune_len=20)  
                sMatrix = np.matrix(som.codebook.matrix)
                sMatrix[np.isnan(sMatrix)] = 0
            else:
                # som.train(n_job=1, verbose=None, train_len_factor=2)
                som.train(n_job=1, verbose=None, train_len_factor=2)

                sMatrix = np.matrix(som.codebook.matrix)
                sMatrix[np.isnan(sMatrix)] = 0
        
        scaler = MinMaxScaler()
        scaler.fit(sMatrix)
        sMatrix = scaler.transform(sMatrix)
        #DC.8. Above ensures that the values in the matrix returned are scaled to between 0 and 1.
        
        tempTopics = getTopics(myNfeatures, myNtopics, 0.7, sMatrix, featureMapping)
        #***************************Get the topics and assign tweets to them ************************
        prevFeatureMap = dict(featureMapping)
        FirstSOM = False
        gv.globalTopics[date_] = AssignTweets(tempTopics, indo_EngTweetsPdCur)
        
    
    f = open(os.path.join(bfgvFolderLoc, "som.pkl"),"wb")
    pickle.dump(som,f)
    f.close()
    #DC.9. Above stores the trained som
    
    tempTopicAgg_norm =dict()
    indo_Engdaily_sentiment_Agg_norm = dict()
    for theDateKey in dateList:
        tempTopicAgg =dict()
        for theTopic in gv.globalTopics[theDateKey]:
            tweetsInTopic = gv.globalTopics[theDateKey][theTopic][1]
            
            for twtInd in tweetsInTopic:
                tweet = indo_EngTweetsPd_sent.loc[twtInd]
                i=0
                j=0
                senti = 0
                sLoc = headerMap['sentiment']-1 
                #DC.10. Above, we subtract 1 because we are accessing each tweet without the column "index" 
                #so naturally the headermap reference has to be shifted by 1- There is perhaps a better way to
                #handle this - I leave as is for now.
                if tweet[sLoc] < 0.0000000000: senti=tweet[sLoc]*-1 
                    # DC.11. this is just to change the 
                    #sentiment value sign to positive (just for ease of calculation) Later if we 
                    #need to consider both negative and positive sentiments,we might have to avoid 
                    #doing this. And keep negative sentiments negative and positive sentiments positive.
                if tweet[sLoc] < 0.0000000000: i=i+1 
                    #DC.12. Above counts the negative sentiments
                if tweet[sLoc] >= 0.0000000000: j=j+1 
                    #DC.13. Above counts the positive and neutral sentiments
                
                if theTopic not in tempTopicAgg.keys(): tempTopicAgg[theTopic]=[0,0,0,0]
                tempTopicAgg[theTopic]=[tempTopicAgg[theTopic][0] + senti,
                                        tempTopicAgg[theTopic][1] + j, tempTopicAgg[theTopic][2] + i,0]

        indo_Engdaily_sentiment_Agg[theDateKey] = tempTopicAgg
        temp1=dict()
        tDicObj =dict()
        for tkey in indo_Engdaily_sentiment_Agg[theDateKey].keys():
            if (indo_Engdaily_sentiment_Agg[theDateKey][tkey][1] + 
                indo_Engdaily_sentiment_Agg[theDateKey][tkey][2]) ==0:
                #DC.14. The sum checks if the total number of ttweets is less 0, if so we set it to 
                # to avoid division by 0.
                x = 1
            else: 
                x = (indo_Engdaily_sentiment_Agg[theDateKey][tkey][1] +
                     indo_Engdaily_sentiment_Agg[theDateKey][tkey][2])
            
            if indo_Engdaily_sentiment_Agg[theDateKey][tkey][2] ==0:
                xi = 1
           # print(x)
            else: 
                xi = indo_Engdaily_sentiment_Agg[theDateKey][tkey][2]
            #DC.15. The above gets the number of negative tweets
            t=0
            
            t=xi/x
            print('++++++++++++++++++++++')
            print(t)
            print('++++++++++++++++++++++')
            

            #DC.15. The above gives us the polarity ratio ofe number of negative tweets to total 
            #negative + positive tweets
            temp1[tkey]=[indo_Engdaily_sentiment_Agg[theDateKey][tkey][0]/xi, t,0,0]
            #DC.16. Above calculates the average sentiment for that topic.
            
            if temp1[tkey][0]>0:
                #DC. 17. We only create an evidence object if the sentiment is bad (>0) otherwise we dont 
                newobj = bc.Evidence(theDateKey,temp1[tkey][0],temp1[tkey][1], x)
                tDicObj[tkey]=newobj
        dicObj.update({theDateKey:tDicObj})
    return dicObj
#######################################################


#######################################################
#10
#This block contains an event read which puts all the events into a dictionary. So the dictionary 
# has a key for all the days under observation and the value is a list where the first index of the list
#holds the binary info of an event and the 2nd postion holds(will hold) the calculated emerging riskScore
def GSREventRead(dicOfEvents, dateList):
    """
    This function takes the stored gsr events (dicOfEvents). This is stored in the format
    {date:[x,y]}, where x is a binary value to indicate that an event occured whilst y indicates
    the extent of the event (not being used atm). Since the gsr events only store the dates that
    are relevant, this functions adds all the other dates within the period with an event value (x) of
    0 to indicate there were no events. we do this just for consitency and for the visualisation - 
    my feeling is that there may be a better way to do this.
    """
    GSREvents = dicOfEvents

    for dated in dateList:
        if dated not in GSREvents.keys():
                GSREvents[dated]=[0,0]
    #GR.1. We dont need to return any value since the update occurs in the referenced value from the 
    #calling function.
    #return (GSREvents)
###########################################################
###########################################################
#11
#This block contains a function that matches words to find similarity based on the sequence of words
#It returns a value between [0-1]. Generally(according to the api notes) values above 0.6 show good similarity
def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()

from datetime import timedelta, date

def daterange(start_date, end_date, interval):
    """
    This function returns the dates between the start and end dates based on the intervals
    ------------------------------------------------------------------------------------
    param start_date/end_date : This is theh start and end dates to be considered
    param interval : This is the interval between each date. Can be 'daily' or it is 
    hourly.
    """

    if interval == "daily":
        for n in range(int ((end_date - start_date).days)):
            yield start_date + timedelta(n)
    else: #DR. 1. This is hourly
        for n in range(int (((end_date - start_date).seconds//3600))):
            yield start_date + timedelta(hours = n)
    

#start_date = date(2013, 1, 1)
#end_date = date(2015, 6, 2)
#dateList = []
#for single_date in daterange(start_date, end_date):
    #dateTup = (single_date.year,single_date.month,single_date.day )
 #   dateList.append(single_date.strftime("%Y-%m-%d"))
    #print(str(dateTup))
#print (dateList)
######################################################################

#######################################################################
#12
#This block contains code that allows us to build a wordcloud from a text
#I no longer use this- to use this you need to have a c compiler
# I have a few notes on these somewhere in my old code
def WCgen(text):
    import numpy as np
    from PIL import Image
    from os import path
    import matplotlib.pyplot as plt
    import random

    from wordcloud import WordCloud, STOPWORDS


    def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
        return "hsl(0, 0%%, %d%%)" % random.randint(60, 100)

    text = text

    wc = WordCloud(max_words=1000, stopwords=stopwords, margin=10, random_state=1).generate(text)
    # store default colored image
    default_colors = wc.to_array()
    plt.title("Daily Summary")
    plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3), interpolation="bilinear")
    #wc.to_file("a_new_hope.png")
    plt.axis("off")
    plt.show()
    
#########################################

##############################################
#13
#This block is important to get the topics of a day
#I will put a few more details here later-MIKE!!!

def getTopics(n_features, n_topics, thresh,sMatrix,featureMapping,cutoff=0.5):
    """
    This primarily takes as input the codebook matrix from som and then applies the LDA on this matrix to 
    get the topics. Using som seems to improve the results - Formal relationship needs to be defined(research) 
    ---------------------------------------------------------------------------------------------------
    param n_features***: This is the number of features to be used in the LDA analysis. Since we have already 
    predefined the features in the som part, we basically pass in the same number of features used in the som
    
    param n_topics***: This is the number of topics to be discovered by lda.
    
    param thresh***: This relates to how the keywords for each topic is selected. After the words relating to
    a topic are discovered by lda, I use properties of the distribution to extract the top keywords. One of
    the properties is the value at the apex of the distribution. we select words that are a certain percentage
    of this apex value. The thresh paramter specifies this ercentage. eg if a the apex value is 100, we select
    words that have a support of at least 70 for a thresh = 0.7 ie (100*0.7)
    
    param smatrix: This is a matrix on which the lda can be applied. The matrix is obtained from the
    som codebook. 
    
    param featuremapping : This is a dictionary of the form {word:index} which is the word and the assigned
    index value.
    
    param cutoff*** (default = 0.5) : This is the cosine similarity value threshold, that we use to determine
    whether the topics are similar before we merge them ie. if two topics have a cosine similarity of more
    than the cutoff, then they are merged.
    
    *** - These parameters can affect the result of the analysis - Formal definitions required
    
    KnownIssues: At the moment we use simple cosine similarity to get the difference between topics. Perhaps
    a different simlarity score such as that of the d2d datascience team will be better. Any score will need
    to be scaled between 0 and 1. A parameter can be put in the fucntioon that specifies what distance
    function to use.
     
    """
    
    ldaTopics = ldamodelling(n_topics, thresh,sMatrix,featureMapping)
    ldaTopicDic =dict()


    #*************************Binarise the topic keywords**************************************
    #The following binarises the topics' keywords
    for each in ldaTopics:
        gv.updateTopicFeatureMap(each)

    theTopics = dict()
    topNo = 0
    for each in ldaTopics:
        temp = list([0]*len(gv.topicFeatureMap))
        for every in each:
            temp[gv.topicFeatureMap[every]] = 1
        theTopics[("T"+str(topNo))] = temp
        #GT.1. Above assigns the binary vector temp to the topic to be used later in distance. 
        ldaTopicDic[("T"+str(topNo))] = each 
        #GT.2. Above assigns the acyual topic keyword list to the topic, to be returned in the end. 
        topNo += 1
        #GT.3. above updated the assigned topic serial numbers.

    #**********************************************************************************************
    keys = list(theTopics.keys())
    masterDic = dict()
    for i in range(len(keys)):
        tempDic =dict()
        for j in range(i+1, len(keys)):
            xarray = np.array(theTopics[keys[i]], ndmin=2)
            xarray1 = np.array(theTopics[keys[j]], ndmin=2)
            x = xarray
            y = xarray1
            tempDic[keys[j]] = cosine_similarity(x,y)[0][0]
        masterDic[keys[i]] = tempDic 
        #GT.4. Above, for the topic corresponding to keys[i], stores a list of all
        #the cosine similarity values for all the topics that where compared to it.

    delList = set()
    for u in masterDic.keys():
        for a in masterDic[u].keys():
            if masterDic[u][a] >= cutoff: 
                ldaTopicDic[a] = list(set(ldaTopicDic[u] + ldaTopicDic[a]))
                delList.add(u)

    for any1 in delList:
        del(ldaTopicDic[any1])
        #GT.5. Above allows us to get rid of all topics merged.
        
    ldaTopics = [ldaTopicDic[k] for k in ldaTopicDic.keys()]
    # print(ldaTopics)

    return(ldaTopics)


#############################################
#####################################
#14
# Here we want to save the topics as a dictionary holding the topickey as key and a list [keywords,tweetid]'
# as the values. This is for a single day 
#AssignTweets - We assign the tweets to a topic but first, we compare the topics to the historical
#topics to see if they are close, if so then we put them together, else we create a new topic.(cutoff)
# Similarly we compare the tweets to the topics to see if they are close and then assign the tweets to 
# the topics and the proceed with the daily calc of the scores.(cutoff1)
def AssignTweets(Topics, indo_EngTweetsPdCur, cutoff1=0.5, cutoff=0.5):
    """
    This assigns the tweets to the topics found. It uses the cosine similarity between the tweets and the
    topics. This is an overlapping assignment, which means that the tweets can belong to multiple topics.
    ---------------------------------------------------------------------------------------------------
    param Topics: These are the topics that have been identified for the day (or period)
    
    param indo_EngTweetsPdCur: These are the tweets for the day (or period) that are going to
    be assigned to the topics.
    
    param cutoff1 (default = 0.5): This the cutoff used when determining whether a tweet belongs to a 
    topic or not so what we do is that we compare the tweet represented in the same vector space as the topic
    and if there is a match such that the match (similarity) exceeds the cutoff then we say that the
    tweet belongs to that topic. a tweet can  belong to multiple topics.
    
    param cutoff: This is the cutoff used to compare the current topics to all previously discovered 
    topics and if there is a match we merge them. we assign the current topic to the previous topic that had
    the highest match, provided the match score exceeds the threshold cutoff.
    
    KnownIssues: At the moment we use simple cosine similarity to get the difference between topics. Perhaps
    a different simlarity score such as that of the d2d datascience team will be better. Any score will need
    to be scaled between 0 and 1. A parameter can be put in the fucntioon that specifies what distance
    function to use.
    """
    DayTopics = dict()
    lda = [gv.globalTopics[key] for key in gv.globalTopics.keys()]
    binaryTopics = dict()
    
    #************** Compares the current topics to all the previous topics to merge**********
    for topic in Topics:
        
        topicKey = "T" + str(gv.theTopicNo)
        gv.theTopicNo += 1
        topicDayTweets = list()
        tempDic = dict()
        
        topicTest= list([0]*len(gv.topicFeatureMap))
        for everyW in topic:
            topicTest[gv.topicFeatureMap[everyW]] = 1
        
        xarray1 = np.array(topicTest, ndmin=2)               
        
        for u in range(len(lda)):
            theKeys = lda[u].keys()
            ldaTopics = [lda[u][ekey] for ekey in theKeys]
            
            theTopics = dict()
            for ekey in theKeys:
                temp = list([0]*len(gv.topicFeatureMap))
                for every in lda[u][ekey][0]:
                    temp[gv.topicFeatureMap[every]] = 1
                theTopics[ekey] = temp
            
            keys = list(theTopics.keys())
            tempDic = dict()
            for i in range(len(keys)):
                xarray = np.array(theTopics[keys[i]], ndmin=2)
                x= xarray
                y= xarray1
                calc = cosine_similarity(x,y)[0][0]
                
                if keys[i] in tempDic.keys():
                    if calc > tempDic[keys[i]]:
                        tempDic[keys[i]] = calc
                else:
                    tempDic[keys[i]] = calc
                
            mtchKey = max(tempDic.items(), key=operator.itemgetter(1))[0]
            mtchNo = max(tempDic.items(), key=operator.itemgetter(1))[1]
            if mtchNo >= cutoff:
                   topicKey = mtchKey
        #*******************************************************************************************    
        #*****************This part assigns tweets of that day to topics***************************
        for i in range(indo_EngTweetsPdCur.count()[0]):
            #AT.1. The above just gets the number of tweets that exist in the dataset.
            numMatch = 0
            eachTweet = indo_EngTweetsPdCur.iloc[i].text
            tweetList = eachTweet.split()
            temp = list([0]*len(gv.topicFeatureMap))
            
            for twtWrd in tweetList:
                if twtWrd in gv.topicFeatureMap.keys():
                    temp[gv.topicFeatureMap[twtWrd]] = 1
            
            xarray = np.array(temp, ndmin=2)
            x = xarray
            y = xarray1
            
            calc = cosine_similarity(x,y)[0][0]
            
            if calc >= cutoff1: 
                topicDayTweets.append(indo_EngTweetsPdCur.index[i])
            
        DayTopics[topicKey] = [topic,topicDayTweets]
        # print(DayTopics[topicKey])
    return(DayTopics)

#15
#This part is Jie's work, I need to clean it a bit and then make sure I keep only what we need

#regex patterns
problemchars = re.compile(r'[\[=\+/&<>;:!\\|*^\'"\?%$.@)°#(_\,\t\r\n0-9-—\]]')
url_finder = re.compile(r'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
emojis = re.compile(u'['
    u'\U0001F300-\U0001F64F'
    u'\U0001F680-\U0001F6FF'
    u'\u2600-\u26FF\u2700-\u27BF]+', 
    re.UNICODE)
stop = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
# username = re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\.]))@([A-Za-z]+[A-Za-z0-9]+)')
username = re.compile(r'(@)\w+( )')
# hashtag = re.compile(r'#(\w+)')
redate = re.compile(r'^(?:(?:(?:0?[13578]|1[02])(\/|-|\.)31)\1|(?:(?:0?[1,3-9]|1[0-2])(\/|-|\.)(?:29|30)\2))(?:(?:1[6-9]|[2-9]\d)?\d{2})$|^(?:0?2(\/|-|\.)29\3(?:(?:(?:1[6-9]|[2-9]\d)?(?:0[48]|[2468][048]|[13579][26])|(?:(?:16|[2468][048]|[3579][26])00))))$|^(?:(?:0?[1-9])|(?:1[0-2]))(\/|-|\.)(?:0?[1-9]|1\d|2[0-8])\4(?:(?:1[6-9]|[2-9]\d)?\d{2})$')
reempty = re.compile(r'^$|\s+')

#nltk sets
PUNCTUATION = set(string.punctuation)
STOPWORDS = set(stopwords.words('english'))
STEMMER = PorterStemmer()
LEMMER = WordNetLemmatizer()
tweet_tokenizer = TweetTokenizer()

# this dictionary is to remove those words that are frequent in GSR text but very unlikely in tweets or other social media as signals
#stopwordsGSR = ["hundreds", "gathered"]
# this dictionary is to remove those words that are frequent in GSR text but very unlikely in tweets or other social media as signals


stopwordsGSR = ["amp"]
# stopwordsGSR = ["amp","rt"]

stop2 = re.compile(r'\b(' + r'|'.join(stopwordsGSR) + r')\b\s*')


def lemmaWordsinString(strinput):
    wnl = WordNetLemmatizer()
    keys = re.findall(r"[\w']+", strinput)
    #keys = [wnl.lemmatize(w) for w in keys]
    keys = [wnl.lemmatize(w, 'v') for w in keys] # add 'v' so that 'loving' is 'love', however, still not work for 'could'
    tmp = ' '.join(keys)
    return tmp


#"abc dddd 8ik" to ["abc", "dddd", "8ik"]
def bodytextToList(strinput):    
    keys = re.findall(r"[\w']+", strinput)
    keys = [w for w in keys]
    return keys


# def cleanText_udf(text):
#     """
#     Removes 'bad' characters from a text. Text is required. 
#     """
#     tmp = stop.sub('', problemchars.sub('', emojis.sub('', url_finder.sub('', username.sub('', stop2.sub('', text.lower().strip()))))))
#     tmp = lemmaWordsinString(tmp)
#     #tmp = stop2.sub('')
#     return tmp


def cleanText_udf_new(text):
    tmp = url_finder.sub('', username.sub('', text.lower().strip()))
    tmp = lemmaWordsinString(tmp)
    # print(tmp)
    return tmp
#print(cleanText_udf_new("i am goona find him Post https://t.co/V6dqLWENkh"))


##############################################################
#author Jie Chen
# I am using this just to summarise the tweets for a given day- I am interested in the topics
# and the words associated with the topics.
def ldamodelling(n_topics, thresh, sMatrix, featureMapping):
    """
    This returns the topics as a list of wordlists. we use the ldamodelling package from sklearn 
    on top of the matrix that we generate from som - see sklearn for details
    --------------------------------------------------------------------------------------------
    param n_topics***: This is the number of topics we want to extract.
    
    param thresh***: This is the threshold for getting the important topic words. It means that
    words that have a support greater than thresh*ss_max (where ss_max is the maximum supportin the dist)
    
    param sMatrix: this is the matrix on which the lda will be applied on. Derived from the SOM.
    
    param featureMapping: This is the mapping betweing vocabulary and the feature column indexes.
    
    *** - These parameters affect the results of the analysis - needs to be formally defined.
    *** - The other parameters in LatentDirichletAllocation also may affect the results
    """
    tf = sMatrix
    
    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10, learning_method='online',
                                    learning_offset=10, random_state=0).fit(tf)
    #LDA.1. Above we use the default - best practice parameters

    tf_feature_names = featureMapping
    topic_words = list_top_words(lda, tf_feature_names, thresh)
    # print('+++++++++++++++++++++topic_words++++++++++++++++++++')
    # print(topic_words)
    
    return topic_words


def print_top_words(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(topic.argsort())
        print("Topic #%d:" % topic_idx)
        print([(feature_names[i], topic[i]) for i in topic.argsort()[:-n_top_words - 1:-1]])
    print()


def list_top_words(model, feature_names, thresh = 0.7, stdTopic=0.1):
    """
    This returns the top words for each topic. Based on novel idea that topics whose word distribution
    are 'informative' are better. By informative we mean that the distribution of the words is non-uniform.
    In this regard we use the parameters stdTopic (standard deviation) and the thresh (the percentage of
    the maximum support that a word should have to be listed as a topic word.
    --------------------------------------------------------------------------------------------------
    param model: This is the ldamodel that has been trained. For each time interval (frequency eg daily) we 
    train a new lda model.
    
    param feature_Names: This is the list of features in order. That is obtained from the feature mapping in
    the calling function.
    
    param thresh***: This is the threshold for getting the important topic words. It means that
    words that have a support greater than thresh*ss_max (where ss_max is the maximum supportin the dist)
    
    param stdTopic*** (default = 0.1): This lets us get the topics that have some level of discrimination 
    in the word distribution. ie. we dont like topics which have a uniform distribution for all the words. 
    So we use the standard deviation as a means to filter topics. Specifiying this can be tricky 
    
    *** - These parameters affect the results of the analysis - needs to be formally defined.
    """
    list_topic = []
    
    for topic_idx, topic in enumerate(model.components_):
        list_mini_topic = []
        maxSupp = max(topic)
        allList = [x/maxSupp for x in topic]
        theSD = np.std(allList)
        AvgSupp = np.mean(topic)
        #chisqP = cs(f_obs=topic)[1]
        #print("Line0", "SD", theSD,"SDThresh = 0.1","avg", AvgSupp )
        if theSD >= stdTopic:
            for i in topic.argsort()[:-len(topic)  - 1:-1]:
                if topic[i]>= (maxSupp*thresh):
                    list_mini_topic.append(feature_names[i])
            list_topic.append(list_mini_topic)
    if not list_topic: list_topic = [["NONE"]]   
    return list_topic


def list_top_words_V1(model, feature_names, n_top_words):
    list_topic = []
    
    for topic_idx, topic in enumerate(model.components_):
        list_mini_topic = []

        for i in topic.argsort()[:-n_top_words - 1:-1]:
            list_mini_topic.append(feature_names[i])
        
        list_topic.append(list_mini_topic)
        
    return list_topic


def list_top_words_V0(model, feature_names, n_top_words):
    list_topic = []
    
    for topic_idx, topic in enumerate(model.components_):
        list_mini_topic = []

        for i in topic.argsort()[:-n_top_words - 1:-1]:
            list_mini_topic.append((feature_names[i], topic[i]))
        
        list_topic.append(list_mini_topic)
        
    return list_topic

def TfidfVectorizerMike(GSR_text, n_features):
#We build a vocabulary here properl
    numberofTerms = n_features
    tf_vectorizer = TfidfVectorizer(input='content',
                                        #max_df=0.95, 
                                        #min_df=10,
                                        ngram_range = (2,3),
                                        max_features=n_features*2,
                                        stop_words='english')

    tf = tf_vectorizer.fit(GSR_text)
    topMosttfterms=tf.get_feature_names()
    topMostidf=tf_vectorizer.idf_.argsort()
    topMostidf = topMostidf[::-1]
    vocab = [topMosttfterms[x] for x in topMostidf[:numberofTerms]]
    tf_vectorizer = TfidfVectorizer(input='content',
                                    #max_df=0.95, 
                                    #min_df=2,
                                    #max_features=10,
                                    vocabulary =vocab,
                                    stop_words='english')

    return tf_vectorizer.fit_transform(GSR_text)


def TfidfVectorizerMike1(GSR_text, n_features):
   """
   This transforms the day's corpus (GSR_text) into a matrix and returns this matrix along with the vocabulary
   i.e. a list of the feature names.Here we vectorizer functions from Sklearn, namely CountVectorizer and
   tfidfVectorizer to convert the text to a matrix. See sklearn for more details on this.
   ------------------------------------------------------------------------------------------------------
   param GSR_text : this is the corpus of tweets as a list of the format [tweet_1,...,tweet_n] to be converted
   to a matrix. 
   
   param n_features*** : this is the number of features to be extracted from the corpus and used to construct
   the matrix.
   
   *** - these parameters affect the results
   *** - the parameters in tf_vectorizer1 can affect the results especially ngram_range
       - the tfidf weighting might also affect the analysis
   """
   tf_vectorizer1 = CountVectorizer(input='content',
                                        #max_df=0.95, 
                                        #min_df=2,
                                        ngram_range = (1,3),
                                        #max_features=10000,
                                        stop_words='english')
    #***the parameters in tf_vectorizer1 can affect the results especially ngram_range******
  
   tf1 = tf_vectorizer1.fit_transform(GSR_text)
   # print(tf_vectorizer1.get_feature_names())
    
   tf1dense = tf1.todense()
   sumtf = np.sum(tf1dense, axis=0)
   sumtf = np.squeeze(np.asarray(sumtf))
    #TF.1. The above gives the tf values for the features 
   tf_vectorizer = TfidfVectorizer(input='content',
                                    #max_df=0.95, 
                                    #min_df=2,
                                    ngram_range=(1,3),
                                    #max_features=10000,
                                    stop_words='english')
   tf = tf_vectorizer.fit_transform(GSR_text)
   # print(tf_vectorizer1.get_feature_names())
   newIDF = tf_vectorizer.idf_
   #TF.2. The above gives the idf values for the features
  
   idftfScores = [newIDF[i]*sumtf[i] for i in range(len(newIDF)) ]
   #TF.3. The above multiplies the tf and idf to give me the actual tfidf values** Potential issue is the multiplication can be mismatched. It doesnt seem like this will happen though.   
    
   Allterms = tf_vectorizer.get_feature_names()
   idftfScores = np.array(idftfScores)# we are here
   topMostidf = idftfScores.argsort()
   topMostidf = topMostidf[::-1]
   vocab = [Allterms[x] for x in topMostidf[:n_features]]
   tf_vectorizer = TfidfVectorizer(input='content',
                                    #max_df=0.95, 
                                    #min_df=2,
                                    #max_features=10,
                                    vocabulary =vocab,
                                    stop_words='english')
        #print("vocab", vocab)
    #print(tf_vectorizer.fit_transform(GSR_text))
   # print("original", vocab)
   return (tf_vectorizer.fit_transform(GSR_text), vocab)
    
    

# Here we will put all these functions into a a class called tools and call the functions that we need 
# We will then import it by the main run file.
#Defined functions
#################################
#This block contains two functions for getting data, depending on where it is stored
def getData_1(date_, source='hdfs:///training/signals-mvpoc/freeport/documents/gnip_v0.1_by_local_date.json'):
    publicationYear = date_[0]
    publicationMonth = date_[1]
    publicationDay = date_[2]
 
    datapath = source + '/publicationYear=' + str(publicationYear) #+'/publicationMonth='+ str(publicationMonth) +'/publicationDay=' + str(publicationDay)  
    tweetsDF = sqlContext.read.json(datapath)
    return(tweetsDF)


def getData_2(source):
    """
    This simply reads in .pkl files
    --------------------------------------------------
    param source : This is the path to the file to be read in.
    """
    return(pd.read_pickle(source))
##############################################################

##############################################################
#7
#This block contains a function to calculate the sentiments of a set of tweets given as a dataframe
#note that the position of the text is very important - this could be improved i think
def sentiCalc1(dataframe):
    """
    This function returns a new dataframe of tweets from the old with the added column sentiment
    """
    indo_EngsentEnd = dataframe.count()[0] # this just gets the number of tweets there are
    #print(indo_EngsentEnd)
    indo_EngTweetsPd_sent = dataframe.copy()
    indo_EngTweetsPd_sent["sentiment"] = ""
    #indo_Engsentences = [dataframe.iloc[x][2] for x in range(0,indo_EngsentEnd)]       
    #indo_EngsentimentValues = []
    # sid = SentimentAnalyser()


    ## SC change ##
    # sid = SentimentAnalyzer()
    sid = SentimentIntensityAnalyzer()

    labels = [indo_EngTweetsPd_sent.index[c] for c in range(indo_EngsentEnd)]
    for f in labels:
        tense = cleanText_udf_new(indo_EngTweetsPd_sent.loc[f].text)
        ss = sid.polarity_scores(tense)  # this returns a dictionary with 4 keys, compound (an overall combination), negative, neutral and postive values
             
        # theSent = ss["compound"]         # note neg, neu, pos, compound
        # if theSent < 0.4:
        #     # theSent =0

        #     indo_EngTweetsPd_sent.loc[f,'sentiment'] =(theSent-1)
        #     indo_EngTweetsPd_sent.loc[f,'text'] = tense
        # else:
        #     indo_EngTweetsPd_sent.loc[f,'sentiment'] =(theSent)
        #     indo_EngTweetsPd_sent.loc[f,'text'] = tense

        theSent = ss["neg"]  
        if theSent < 0.4:
            theSent = 0
        indo_EngTweetsPd_sent.loc[f,'sentiment'] = (theSent*-1)
        indo_EngTweetsPd_sent.loc[f,'text'] = tense


    #indo_EngTweetsPd_sent = dataframe.copy()
    #indo_EngTweetsPd_sent["sentiment"] = Series(indo_EngsentimentValues)
    return(indo_EngTweetsPd_sent)

###########################################################
##############################################################

def sentiCalc(dataframe):
    "This function returns a new dataframe of tweets from the old with the added column sentiment"
    indo_EngsentEnd = dataframe.count()[0] # this just gets the number of tweets there are
    #print(indo_EngsentEnd)
    indo_EngTweetsPd_sent = dataframe.copy()
    indo_EngTweetsPd_sent["sentiment"] = ""
    
    #indo_Engsentences = [dataframe.iloc[x][2] for x in range(0,indo_EngsentEnd)]       
    #indo_EngsentimentValues =[]
    sid = SentimentIntensityAnalyzer()
    labels = [indo_EngTweetsPd_sent.index[c] for c in range(indo_EngsentEnd)]
    for f in labels:
        tense = cleanText_udf_new(indo_EngTweetsPd_sent.loc[f].text)
        tense = translateInd1(tense)
        ss = sid.polarity_scores(tense) # this returns a dictionary with 4 keys, compound (an overall combination), negative, neutral and postive values
        indo_EngTweetsPd_sent.loc[f,'sentiment'] =ss["compound"]
        indo_EngTweetsPd_sent.loc[f,'text'] = tense

    #indo_EngTweetsPd_sent = dataframe.copy()
    #indo_EngTweetsPd_sent["sentiment"] = Series(indo_EngsentimentValues)
    return(indo_EngTweetsPd_sent)

###########################################################
############################################################
#8
#This block contains a function for calculating the emerging riskScore based on previous GSR Events
# I dont really use this any longer
def GSRThresh(GSREvents, window, startDate, endDate,prev_thresh):
    keys = GSREvents.keys()
    odKeys = sorted(keys)
    nsCk = window-1
    #first = True
    ini_thresh =0.001
    #prev_thresh = []
    for i in range(len(odKeys)):
        if i<=nsCk:
            nearScore = ini_thresh
        else:
            #print([combined_dict[odKeys[x]][3] for x in range(i-nsCk,i+1)])
            nearScore = max([dicObj[odKeys[x]].riskScore1 for x in range(i-nsCk,i+1)]) #last three days max
        if (GSREvents[odKeys[i]][0] == 0) & (len(prev_thresh) == 0):
            GSREvents[odKeys[i]][1] = ini_thresh
        #prev_thresh.apend(combined_dict[key][5])
        
        elif (GSREvents[odKeys[i]][0] == 0) & (len(prev_thresh) != 0):
            GSREvents[odKeys[i]][1] = sum(prev_thresh)/len(prev_thresh)
        elif (GSREvents[odKeys[i]][0] > 0) & (len(prev_thresh) == 0):
            GSREvents[odKeys[i]][1] = nearScore
            prev_thresh.append(nearScore)
        #print(odKeys[i], prev_thresh)
        else :
            prev_thresh.append(nearScore)#last three days max
            GSREvents[odKeys[i]][1] = sum(prev_thresh)/len(prev_thresh)
    return prev_thresh
##############################################################

################################################################
#9
#This block contains a function that takes the sentiments of the tweets and then creates 
#a new evidence object which is eventually returned as an object - in fact the final return statement
#is a dictionary of objects, where the key is the day and the value is the object that holds the evidence
#This has since been improved to include the topics.i.e. {Day:{Topic:object}}
def filterSpurious(dicObj, searchWindow=None,noOccurrence=1 ):
    """
    We use this to filter the spurious topics so that the visualisation is not marred. The spurious
    topics are topics that appear less than "noOccurrence" times within a window of "searchWindow".
    The filtered out dictionary is returned.
    -------------------------------------------------------------------------------
    param dicObj: This is the dictionary of topics from which we will be filtering out spurious.  
    
    param searchWindow (default = None): This is the number of days after the current date whose 
    topics are under consideration will be considered when searching for the no of occurence. 
    This means that the whole length of the dates will be searched.
    
    param noOccurrence (default = 1): This is the number of occurence below which a topic is 
    considered spurious.The default 1 means so far as a topic appears that will be enough.
    """
    dicDates = list(dicObj.keys())
    AllTopics = list()
    for c in range(len(dicDates)):
        AllTopics.append(list(dicObj[dicDates[c]].keys()))
    intersectList = dict()
    if searchWindow == None: searchWindow= len(AllTopics)
    for c1 in range(len(AllTopics)):
        CdateTopics = AllTopics[c1]
        for c2 in range(c1+1, min(searchWindow+c1+1, len(AllTopics))):
            FdateTopics=  AllTopics[c2]
            for value in CdateTopics:
                if value not in intersectList.keys(): intersectList[value] = 1
                if value in FdateTopics:
                    intersectList[value] = intersectList[value]+1
                    AllTopics[c2].remove(value)
    
    if noOccurrence==None: noOccurrence= 1
    filteredDicObj = copy.deepcopy(dicObj)
    for c in range(len(dicDates)):
        
        for tkey in list(filteredDicObj[dicDates[c]].keys()):
            if tkey not in intersectList.keys() or intersectList[tkey] < noOccurrence:
                del filteredDicObj[dicDates[c]][tkey]
    return(filteredDicObj)  


def getDetails(aDate, dicObj, gvFolderLoc, topic=None, details="summary", normConst=1):
    """
    This gets the summary for a given date and/or topic. The summary includes the object
    called summary and in addition the tweetIds of teh tweets associated with the topic
    under consideration.
    ---------------------------------------------------------
    param aDate : This is the date under consideration
    
    param dicObj: This is the dictionary of evidence objects that is read in.
    
    param gvFoldLoc: This is the folder where all the global variables are stored.
    
    param topic (default = none): This the topic under consideration. If none, all the 
    topics for the day will be summarised.
    
    param details (default = "summary"): is either summary or None. If none then the 
    details will be returnd as an object which may be used for further processing
    """
    globalTopics = getData_2(os.path.join(gvFolderLoc, "globalTopics.pkl"))
    theTopics = dicObj[aDate]
    if topic is None:
        if details == "summary":
            print("The evidence for this day", aDate, " is as follows: ")
            for eachTopic in theTopics.keys():
                print("Topic", eachTopic) 
                print("Score : ", theTopics[eachTopic].riskScore1/normConst)
                print(theTopics[eachTopic].summary)
                #print("keyWords : ", globalTopics[aDate][eachTopic][0])
                print("TweetIds : ", globalTopics[aDate][eachTopic][1])
                print("_______________________________________________")

        else:
            return(globalTopics[aDate])
            
    else:
        if details == "summary":
            print("The evidence for this day ", aDate, " and topic ", topic, " is as follows: ")
            print("Topic", topic) 
            print("Score : ", theTopics[topic].riskScore1/normConst)
            print(theTopics[topic].summary)
            #print("keyWords : ", globalTopics[aDate][eachTopic][0])
            print("TweetIds : ", globalTopics[aDate][topic][1])
            print("_______________________________________________")

        else:
            return(globalTopics[aDate][topic])
    
    

